# Vision Transformer (ViT) from Scratch

This repository provides a clean and minimal implementation of the **Vision Transformer (ViT)** architecture from scratch using  **PyTorch** . It is designed for learning and experimentation with the Transformer-based approach to image classification, without relying on pre-built ViT modules.

---

## ğŸ“Œ Features

* âœ… Vision Transformer (ViT) architecture implemented from first principles
* âœ… Patch embedding using a simple linear projection
* âœ… Multi-head Self-Attention mechanism
* âœ… Position embeddings
* âœ… Transformer encoder blocks with LayerNorm and residual connections
* âœ… Classification head for image classification
* âœ… End-to-end training pipeline using PyTorch
* âœ… DataLoader with augmentation for classification

---

## ğŸ“ Directory Structure

```
Vision-Transformer-From-Scratch/
â”œâ”€â”€ datasets/                             # Dataset handling utilities
â”‚   â”œâ”€â”€ classification_dataset.py         # Custom Dataset class for image classification tasks
â”‚   â””â”€â”€ __init__.py                       # Package initializer
â”‚
â”œâ”€â”€ models/                               # Model architecture components
â”‚   â”œâ”€â”€ vit.py                            # Vision Transformer (ViT) wrapper combining all modules
â”‚   â””â”€â”€ modules/                          # Core building blocks of the ViT model
â”‚       â”œâ”€â”€ attention.py                  # Implements scaled dot-product attention
â”‚       â”œâ”€â”€ multihead_attention.py        # Implements multi-head self-attention
â”‚       â”œâ”€â”€ patch_embedding.py            # Splits image into patches and embeds them
â”‚       â”œâ”€â”€ positional_embedding.py       # Adds positional information to patch embeddings
â”‚       â””â”€â”€ transformer_encoder.py        # Transformer encoder block with attention and MLP
â”‚
â”œâ”€â”€ trainers/                             # Training logic
â”‚   â”œâ”€â”€ classification_trainer.py         # Training loop and evaluation logic for classification
â”‚   â””â”€â”€ __init__.py                       # Package initializer
â”‚
â”œâ”€â”€ utils/                                # Helper utilities
â”‚   â””â”€â”€ utils.py                          # General-purpose utility functions (e.g., metric calculation, logging)
â”‚
â”œâ”€â”€ config.py                             # Centralized configuration for hyperparameters and paths
â”œâ”€â”€ main.py                               # Entry point for training the Vision Transformer
â”œâ”€â”€ ReadMe.md                             # Project documentation (you're here!)
â””â”€â”€ requirements.txt			  # Required Packages
```

---

## ğŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/ahanaf019/Vision-Transformer-From-Scratch.git
cd Vision-Transformer-From-Scratch
```

### 2. Install dependencies

We recommend using a virtual environment.

```bash
pip install -r requirements.txt
```

### 3. Train the model

```bash
python main.py
```

---

## ğŸ§  Model Overview

The Vision Transformer (ViT) follows this architecture:

1. **Image to Patches** : Splits image into fixed-size patches (e.g., 16x16).
2. **Patch Embedding** : Each patch is flattened and projected into a vector.
3. **Position Embedding** : Positional information is added to retain spatial order.
4. **Transformer Encoder** : A stack of transformer blocks (Multi-Head Attention + MLP).
5. **Classification Head** : A linear layer for final prediction using the `[CLS]` token.

---

## âš™ï¸ Configuration

You can customize training parameters via `config.py`:

```python
IMAGE_SIZE = 32
PATCH_SIZE = 4
NUM_CLASSES = 10
DIM = 256
DEPTH = 6
HEADS = 8
MLP_DIM = 512
EPOCHS = 30
BATCH_SIZE = 64
LEARNING_RATE = 3e-4
```

---

## ğŸ§ª Dataset

This implementation uses **STL-10** by default, which should be already downloaded inside `DB_PATH`.

To switch datasets, modify the `datasets/classification_dataset.py` file accordingly.

---

## ğŸ™Œ Acknowledgements

Implemented from the original [ViT paper by Dosovitskiy et al.](https://arxiv.org/abs/2010.11929).
